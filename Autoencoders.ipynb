{"metadata":{"colab":{"provenance":[],"collapsed_sections":["KN3D_k5W_WZz","-NDiCPYLm2bY"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1TRNaCfYstvcIQqoUSdukYQGF6LuyL7Tv\" width=600 height=320/></p>\n<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n\n# Домашнее задание. Автоэнкодеры\n","metadata":{"id":"LQ7i1HkmYY68"}},{"cell_type":"markdown","source":"# Часть 1. Vanilla Autoencoder (9 баллов)","metadata":{"id":"Wru2LNFuL2Iq"}},{"cell_type":"markdown","source":"## 1.1. Подготовка данных (0.5 балла)\n","metadata":{"id":"kr3STtdpYY7G"}},{"cell_type":"code","source":"import numpy as np\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data_utils\nimport torch\nimport matplotlib.pyplot as plt\n\nimport os\nimport pandas as pd\nimport skimage.io\nfrom skimage.transform import resize\nfrom IPython import display\n\n\n%matplotlib inline","metadata":{"id":"xTNi9JLRYY7I","execution":{"iopub.status.busy":"2023-11-27T15:20:05.135968Z","iopub.execute_input":"2023-11-27T15:20:05.136337Z","iopub.status.idle":"2023-11-27T15:20:05.144595Z","shell.execute_reply.started":"2023-11-27T15:20:05.136300Z","shell.execute_reply":"2023-11-27T15:20:05.143663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n                      images_name = \"lfw-deepfunneled\",\n                      dx=80,dy=80,\n                      dimx=64,dimy=64\n    ):\n\n    #download if not exists\n    if not os.path.exists(images_name):\n        print(\"images not found, donwloading...\")\n        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n        print(\"extracting...\")\n        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n        print(\"done\")\n        assert os.path.exists(images_name)\n\n    if not os.path.exists(attrs_name):\n        print(\"attributes not found, downloading...\")\n        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n        print(\"done\")\n\n    #read attrs\n    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,)\n    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n\n\n    #read photos\n    photo_ids = []\n    for dirpath, dirnames, filenames in os.walk(images_name):\n        for fname in filenames:\n            if fname.endswith(\".jpg\"):\n                fpath = os.path.join(dirpath,fname)\n                photo_id = fname[:-4].replace('_',' ').split()\n                person_id = ' '.join(photo_id[:-1])\n                photo_number = int(photo_id[-1])\n                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n\n    photo_ids = pd.DataFrame(photo_ids)\n    # print(photo_ids)\n    #mass-merge\n    #(photos now have same order as attributes)\n    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n\n    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n\n    # print(df.shape)\n    #image preprocessing\n    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n                                .apply(lambda img: resize(img,[dimx,dimy]))\n\n    all_photos = np.stack(all_photos.values)#.astype('uint8')\n    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n\n    return all_photos, all_attrs","metadata":{"id":"zvAjov5F2NvE","execution":{"iopub.status.busy":"2023-11-27T15:20:05.146276Z","iopub.execute_input":"2023-11-27T15:20:05.146561Z","iopub.status.idle":"2023-11-27T15:20:05.163909Z","shell.execute_reply.started":"2023-11-27T15:20:05.146538Z","shell.execute_reply":"2023-11-27T15:20:05.162888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n\n\ndata, attrs = fetch_dataset()","metadata":{"id":"W3KhlblLYY7P","outputId":"74acb49e-10ce-4763-bcc1-f3651f6e2841","execution":{"iopub.status.busy":"2023-11-27T15:20:05.165392Z","iopub.execute_input":"2023-11-27T15:20:05.165817Z","iopub.status.idle":"2023-11-27T15:20:38.855397Z","shell.execute_reply.started":"2023-11-27T15:20:05.165783Z","shell.execute_reply":"2023-11-27T15:20:38.854579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"backup = data.copy()","metadata":{"id":"DpBRWsqtFqU4","execution":{"iopub.status.busy":"2023-11-27T15:20:38.856813Z","iopub.execute_input":"2023-11-27T15:20:38.857336Z","iopub.status.idle":"2023-11-27T15:20:39.113405Z","shell.execute_reply.started":"2023-11-27T15:20:38.857300Z","shell.execute_reply":"2023-11-27T15:20:39.112668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"id":"gXgp6V-fMKog","outputId":"03c6d748-a5bf-4c3e-d109-65012742782e","execution":{"iopub.status.busy":"2023-11-27T15:20:39.115687Z","iopub.execute_input":"2023-11-27T15:20:39.115968Z","iopub.status.idle":"2023-11-27T15:20:39.122019Z","shell.execute_reply.started":"2023-11-27T15:20:39.115944Z","shell.execute_reply":"2023-11-27T15:20:39.121050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nРазбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:","metadata":{"id":"8MSzXXGoYY7X"}},{"cell_type":"code","source":"np.random.shuffle(data)\n\n\ndata_size = data.shape[0]\ntrain_size = data_size // 10 * 7\n\ntrain_data = torch.FloatTensor(data[0:train_size]).to(device)\nval_data = torch.FloatTensor(data[train_size:data_size]).to(device)\n\nprint(train_data.shape)\nprint(val_data.shape)\nprint(data.shape)\n\ntrain_loader = data_utils.DataLoader(train_data, batch_size=64)\nval_loader = data_utils.DataLoader(val_data, batch_size=64)","metadata":{"scrolled":true,"id":"dFc8lTm_YY7Y","outputId":"d99f0208-5fcb-4f28-bc2c-24c3a1944c4c","execution":{"iopub.status.busy":"2023-11-27T17:29:21.246583Z","iopub.execute_input":"2023-11-27T17:29:21.246966Z","iopub.status.idle":"2023-11-27T17:29:21.689966Z","shell.execute_reply.started":"2023-11-27T17:29:21.246935Z","shell.execute_reply":"2023-11-27T17:29:21.688911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nfor i in range(6):\n    plt.subplot(1, 6, i + 1)\n    plt.imshow(train_data[i].cpu())\n    print(train_data[i].shape)","metadata":{"id":"r2x-NCrVMr8E","outputId":"6bd696e0-46ce-440d-fa33-afd04b464560","execution":{"iopub.status.busy":"2023-11-27T15:20:39.656912Z","iopub.execute_input":"2023-11-27T15:20:39.657226Z","iopub.status.idle":"2023-11-27T15:20:40.544535Z","shell.execute_reply.started":"2023-11-27T15:20:39.657200Z","shell.execute_reply":"2023-11-27T15:20:40.543609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Архитектура модели (1.5 балла)\nВ этом разделе мы напишем и обучем обычный автоэнкодер.\n\n\n\n<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">\n\n\n^ напомню, что автоэнкодер выглядит вот так","metadata":{"id":"z9CC-DUhYY7i"}},{"cell_type":"code","source":"dim_code = 16 # выберите размер латентного вектора\nin_feat = 64 * 64 * 3","metadata":{"id":"csrNCYh-YY7j","execution":{"iopub.status.busy":"2023-11-27T15:20:40.545786Z","iopub.execute_input":"2023-11-27T15:20:40.546148Z","iopub.status.idle":"2023-11-27T15:20:40.550938Z","shell.execute_reply.started":"2023-11-27T15:20:40.546114Z","shell.execute_reply":"2023-11-27T15:20:40.550060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!","metadata":{"id":"Fjr-N8AWee-k"}},{"cell_type":"code","source":"from copy import deepcopy\n\nclass Autoencoder(nn.Module):\n    def __init__(self):\n\n        super().__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(in_feat, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, dim_code)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(dim_code, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, in_feat)\n        )\n\n    def forward(self, x):\n\n        x = x.flatten(1, 3)\n\n        x = self.encoder(x)\n        latent_code = x\n\n        x = self.decoder(x).view(-1, 64, 64, 3)\n        reconstruction = torch.sigmoid(x)\n\n\n        return reconstruction, latent_code","metadata":{"id":"8SjHNX-rYY7k","execution":{"iopub.status.busy":"2023-11-27T17:29:04.772740Z","iopub.execute_input":"2023-11-27T17:29:04.773164Z","iopub.status.idle":"2023-11-27T17:29:04.784616Z","shell.execute_reply.started":"2023-11-27T17:29:04.773132Z","shell.execute_reply":"2023-11-27T17:29:04.783730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Обучение (2 балла)","metadata":{"id":"GpntmZCe5L6i"}},{"cell_type":"markdown","source":"Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n\nА, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)","metadata":{"id":"Bdxg_3WJYY7o"}},{"cell_type":"code","source":"from tqdm import tqdm\nimport time","metadata":{"id":"H6VuB5oyLgKb","execution":{"iopub.status.busy":"2023-11-27T17:29:05.344562Z","iopub.execute_input":"2023-11-27T17:29:05.345297Z","iopub.status.idle":"2023-11-27T17:29:05.349457Z","shell.execute_reply.started":"2023-11-27T17:29:05.345264Z","shell.execute_reply":"2023-11-27T17:29:05.348458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nchecker = Autoencoder().to(device)\noptimizer = optim.Adam(autoencoder.parameters())\n\n\nepochs = 15\n\nim_to_show = 5\n\nsample = val_data[torch.randperm(val_data.shape[0])[:5]]\n\nt_losses = []\nv_losses = []\n\nfor epoch in tqdm(range(epochs)):\n    t_l = []\n    v_l = []\n    for k, loader in enumerate([train_loader, val_loader]):\n        for batch in loader:\n            batch = batch.to(device)\n            if k == 0:\n                checker.train()\n                optimizer.zero_grad()\n                pred, lc = checker(batch)\n                loss = criterion(pred.flatten(1,3), batch.flatten(1,3))\n                # print(loss.item())\n                t_l.append(loss.item())\n                loss.backward()\n                optimizer.step()\n\n            if k == 1:\n                with torch.no_grad():\n                    checker.eval()\n                    pred, lc = checker(batch)\n                    loss = criterion(pred.flatten(1,3), batch.flatten(1,3))\n                    v_l.append(loss.item())\n\n\n    t_losses.append(np.mean(t_l))\n    v_losses.append(np.mean(v_l))\n\n#     with torch.no_grad():\n#         autoencoder.eval()\n#         plt.figure(figsize=(10,3))\n#         plt.title('epoch' + str(epoch + 1))\n#         for i in range(im_to_show):\n#             plt.subplot(2, im_to_show, i + 1)\n#             plt.imshow(np.array(sample[i].cpu() * 255).astype(np.uint8))\n#             plt.subplot(2, im_to_show, i + im_to_show + 1)\n#             pr, _ = autoencoder(sample[i][None, :])\n#             plt.imshow(np.array(pr.cpu()[0]))\n\n#         display.clear_output(wait=True)\n#         display.display(plt.gcf())\n\n# display.clear_output(wait=True)\n# plt.figure(figsize=(12,6))\n# plt.plot(range(epochs), t_losses, label='train')\n# plt.plot(range(epochs), v_losses, label='val')\n# plt.legend()","metadata":{"scrolled":true,"id":"3H3DOojrYY7o","outputId":"e9de6bf8-0d32-4147-9064-5889ce657a94","execution":{"iopub.status.busy":"2023-11-27T17:29:27.354610Z","iopub.execute_input":"2023-11-27T17:29:27.355009Z","iopub.status.idle":"2023-11-27T17:29:27.536392Z","shell.execute_reply.started":"2023-11-27T17:29:27.354979Z","shell.execute_reply":"2023-11-27T17:29:27.535103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:","metadata":{"id":"FAztAMA4YY7q"}},{"cell_type":"code","source":"aboba = val_data[torch.randperm(val_data.shape[0])[26:31]]\nplt.figure(figsize=(10,3))\nplt.title('epoch' + str(epoch + 1))\nwith torch.no_grad():\n    autoencoder.eval()\n    for i in range(im_to_show):\n        plt.subplot(2, im_to_show, i + 1)\n        plt.imshow(np.array(aboba[i].cpu()))\n        plt.subplot(2, im_to_show, i + im_to_show + 1)\n        pr, _ = autoencoder(aboba[i][None, :])\n        plt.imshow(np.array(pr.cpu()[0]))","metadata":{"id":"I1J__yvxYY7r","outputId":"cd3b1e75-0b23-4f7f-e6cc-a91aa8992c75","execution":{"iopub.status.busy":"2023-11-27T15:20:50.164362Z","iopub.execute_input":"2023-11-27T15:20:50.165050Z","iopub.status.idle":"2023-11-27T15:20:51.412001Z","shell.execute_reply.started":"2023-11-27T15:20:50.165016Z","shell.execute_reply":"2023-11-27T15:20:51.411101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not bad, right?","metadata":{"id":"1OPh9O6UYY7s"}},{"cell_type":"markdown","source":"## 1.4. Sampling (2 балла)","metadata":{"id":"dFi96giuYY7t"}},{"cell_type":"markdown","source":"Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n\nДавайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n\n__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать.","metadata":{"id":"AOtUaPNYYY7t"}},{"cell_type":"code","source":"torch.normal(mean=1, std=2, size=(16,))","metadata":{"id":"oQz96FTMcEGy","outputId":"bf01a081-04ea-4917-82ce-7c3ce0c91d0e","execution":{"iopub.status.busy":"2023-11-27T15:20:51.413383Z","iopub.execute_input":"2023-11-27T15:20:51.413716Z","iopub.status.idle":"2023-11-27T15:20:51.433494Z","shell.execute_reply.started":"2023-11-27T15:20:51.413687Z","shell.execute_reply":"2023-11-27T15:20:51.432739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elem = val_data[170]\n\nplt.figure(figsize=(2,2))\nplt.imshow(np.array(elem.cpu()))\nplt.show()\n\nautoencoder.eval()\npred, lc = autoencoder(elem[None, :])\nwith torch.no_grad():\n    plt.figure(figsize=(2,2))\n    plt.imshow(np.array(pred[0].cpu()))\n    plt.show()\n\n\nprint(lc)","metadata":{"scrolled":true,"id":"8IZykARRYY7u","outputId":"959eea29-cac6-4ad8-bb69-266c5f2a9c34","execution":{"iopub.status.busy":"2023-11-27T15:20:51.437727Z","iopub.execute_input":"2023-11-27T15:20:51.437974Z","iopub.status.idle":"2023-11-27T15:20:51.851963Z","shell.execute_reply.started":"2023-11-27T15:20:51.437952Z","shell.execute_reply":"2023-11-27T15:20:51.851116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    value = torch.normal(mean=0, std=1, size=(16,))\n    x = value[None, :].cuda()\n    x = autoencoder.decoder(x).view(-1, 64, 64, 3)\n    reconstruction = torch.sigmoid(x)\n    plt.figure(figsize=(2,2))\n    plt.imshow(np.array(reconstruction.cpu()[0]))\n    plt.show()","metadata":{"id":"I9IWQCTwc_Kw","outputId":"44de5d09-7665-4083-994c-715770102b06","execution":{"iopub.status.busy":"2023-11-27T15:20:51.854814Z","iopub.execute_input":"2023-11-27T15:20:51.855153Z","iopub.status.idle":"2023-11-27T15:20:52.057258Z","shell.execute_reply.started":"2023-11-27T15:20:51.855116Z","shell.execute_reply":"2023-11-27T15:20:52.056412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attrs.head()","metadata":{"id":"w9OfEaPBo-0g","outputId":"593c2bbe-1e31-44c7-f080-3463bd293666","execution":{"iopub.status.busy":"2023-11-27T15:20:52.058554Z","iopub.execute_input":"2023-11-27T15:20:52.058818Z","iopub.status.idle":"2023-11-27T15:20:52.081447Z","shell.execute_reply.started":"2023-11-27T15:20:52.058795Z","shell.execute_reply":"2023-11-27T15:20:52.080607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5 Time to make fun! (3 балла)\n\nДавайте научимся пририсовывать людям улыбки =)","metadata":{"id":"Ey8dD9s0YY7w"}},{"cell_type":"code","source":"a = [1,2,3,4]\na[-2:]","metadata":{"id":"baRY2k0hqLlx","outputId":"34f12d3b-392f-4743-dd94-5320b07c8697","execution":{"iopub.status.busy":"2023-11-27T15:20:52.082469Z","iopub.execute_input":"2023-11-27T15:20:52.082722Z","iopub.status.idle":"2023-11-27T15:20:52.088608Z","shell.execute_reply.started":"2023-11-27T15:20:52.082700Z","shell.execute_reply":"2023-11-27T15:20:52.087600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">","metadata":{"id":"i1v-8WwuYY7w"}},{"cell_type":"markdown","source":"План такой:\n\n1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n\nНайти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n\n2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n\n3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n\n4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!","metadata":{"id":"eGE0M2GDYY7x"}},{"cell_type":"code","source":"n_elements = 700\nhalf_n = 350\n\n\nsmile = attrs.sort_values('Smiling')[-n_elements:].sort_values('Male')\nsad = attrs.sort_values('Smiling')[:n_elements].sort_values('Male')\n\nwomen_smile = smile[:half_n].index\nmen_smile = smile[-half_n:].index\n\nwomen_sad = sad[:half_n].index\nmen_sad = sad[-half_n:].index\n\nautoencoder.eval()\nwsm_elems = torch.FloatTensor(backup[women_smile]).to(device)\nmsm_elems = torch.FloatTensor(backup[men_smile]).to(device)\nwsd_elems = torch.FloatTensor(backup[women_sad]).to(device)\nmsd_elems = torch.FloatTensor(backup[men_sad]).to(device)\n\n_, lc_wsm = autoencoder(wsm_elems)\n_, lc_wsd = autoencoder(wsd_elems)\n_, lc_msm = autoencoder(msm_elems)\n_, lc_msd = autoencoder(msd_elems)\n\nmen_diff = torch.mean(lc_msm - lc_msd, -2)\nwomen_diff = torch.mean(lc_wsm - lc_wsd, -2)\n\n\nn_test = 5\n\ntest = attrs.sort_values('Smiling')[n_elements+4:n_elements+4+n_test]\n\ntest_men = torch.FloatTensor(backup[\n    test[test['Male'] > 0].index\n]).to(device)\ntest_women = torch.FloatTensor(backup[\n    test[test['Male'] <= 0].index\n]).to(device)\n\n\npr_m, lcs_m = autoencoder(test_men)\npr_w, lcs_w = autoencoder(test_women)\n\n\nsubjects_m = torch.sigmoid(autoencoder.decoder(lcs_m + men_diff).view(-1, 64, 64, 3))\nsubjects_w = torch.sigmoid(autoencoder.decoder(lcs_w + women_diff).view(-1, 64, 64, 3))\n\nn_m = subjects_m.shape[0]\nn_w = subjects_w.shape[0]\n\nwith torch.no_grad():\n    for i in range(n_m):\n        plt.subplot(3, n_m, i + 1)\n        plt.imshow(np.array(test_men[i].cpu()))\n        plt.subplot(3, n_m, i + n_m +1)\n        plt.imshow(np.array(pr_m[i].cpu()))\n        plt.subplot(3, n_m, i + 2 * n_m + 1)\n        plt.imshow(np.array(subjects_m[i].cpu()))\n    plt.show()\n    for i in range(n_w):\n        plt.subplot(3, n_w, i + 1)\n        plt.imshow(np.array(test_women[i].cpu()))\n        plt.subplot(3, n_w, i + n_w +1)\n        plt.imshow(np.array(pr_w[i].cpu()))\n        plt.subplot(3, n_w, i + 2 * n_w + 1)\n        plt.imshow(np.array(subjects_w[i].cpu()))\n    plt.show()","metadata":{"id":"f1oBX9EeYY7x","outputId":"5c662742-23c9-4f49-c337-0505fd90a856","execution":{"iopub.status.busy":"2023-11-27T15:20:52.089688Z","iopub.execute_input":"2023-11-27T15:20:52.090006Z","iopub.status.idle":"2023-11-27T15:20:54.010827Z","shell.execute_reply.started":"2023-11-27T15:20:52.089983Z","shell.execute_reply":"2023-11-27T15:20:54.009850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вуаля! Вы восхитительны!","metadata":{"id":"bXI6jprOYY7z"}},{"cell_type":"markdown","source":"Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs`:)","metadata":{"id":"E2UAf0bpYY70"}},{"cell_type":"markdown","source":"# Часть 2: Variational Autoencoder (10 баллов)","metadata":{"id":"QQnEGmknYY71"}},{"cell_type":"markdown","source":"Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9","metadata":{"id":"bWQNRjJq2uTz"}},{"cell_type":"code","source":"batch_size = 32\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"qBXXr9njByYC","outputId":"a2063ed5-ddbd-4e86-a493-1d369df089b1","execution":{"iopub.status.busy":"2023-11-27T15:20:54.012013Z","iopub.execute_input":"2023-11-27T15:20:54.012365Z","iopub.status.idle":"2023-11-27T15:20:54.048216Z","shell.execute_reply.started":"2023-11-27T15:20:54.012335Z","shell.execute_reply":"2023-11-27T15:20:54.047281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Архитектура модели и обучение (2 балла)\n\nРеализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре:) Экспериментируйте!","metadata":{"id":"2rHphW5l8Wgi"}},{"cell_type":"code","source":"in_feat = 28 * 28\nlatent_dim = 64","metadata":{"id":"DadxUgrY4ypr","execution":{"iopub.status.busy":"2023-11-27T15:20:54.049372Z","iopub.execute_input":"2023-11-27T15:20:54.050131Z","iopub.status.idle":"2023-11-27T15:20:54.058450Z","shell.execute_reply.started":"2023-11-27T15:20:54.050089Z","shell.execute_reply":"2023-11-27T15:20:54.057509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self):\n\n        super().__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1), # 28 - 28\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d(2), # 28 - 14\n            nn.Conv2d(16, 64, kernel_size=5), # 14 - 10\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2), # 10 - 5\n            nn.Conv2d(64, 128, kernel_size=5), # 5 - 1\n            nn.ReLU(),\n            nn.Flatten(1, 3),\n            nn.Linear(128, latent_dim * 2)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2), # 1 - 3\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2), # 3 - 7\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), # 7 - 14\n            nn.ReLU(),\n            nn.BatchNorm2d(8),\n            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), # 14 - 28\n        )\n\n    def encode(self, x):\n        x = self.encoder(x).view(-1, 2, latent_dim)\n\n        mu = x[:, 0, :]\n        logsigma = x[:, 1, :]\n\n        return mu, logsigma\n\n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            std = torch.exp(0.5 * logsigma)\n            eps = torch.randn_like(std)\n            return eps * std + mu\n        else:\n            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu.\n            # на инференсе выход автоэнкодера должен быть детерминирован.\n            return mu\n\n    def decode(self, z):\n        reconstruction = self.decoder(z.view(-1, 64, 1, 1))\n\n        return reconstruction\n\n    def forward(self, x):\n        mu, logsigma = self.encode(x)\n        sample = self.gaussian_sampler(mu, logsigma)\n        reconstruction = self.decode(sample)\n#         reconstruction = torch.sigmoid(reconstruction)\n        return mu, logsigma, reconstruction","metadata":{"id":"IoNVT5tYYY74","execution":{"iopub.status.busy":"2023-11-27T15:20:54.059923Z","iopub.execute_input":"2023-11-27T15:20:54.060262Z","iopub.status.idle":"2023-11-27T15:20:54.078663Z","shell.execute_reply.started":"2023-11-27T15:20:54.060233Z","shell.execute_reply":"2023-11-27T15:20:54.077754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Определим лосс и его компоненты для VAE:","metadata":{"id":"hAB77d-PYY76"}},{"cell_type":"markdown","source":"Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n\nОбщий лосс будет выглядеть так:\n\n$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n\nФормула для KL-дивергенции:\n\n$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n\nВ качестве log-likelihood возьмем привычную нам кросс-энтропию.","metadata":{"id":"UxJrkXGQo5bp"}},{"cell_type":"code","source":"def KL_divergence(mu, logsigma):\n    \"\"\"\n    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n    \"\"\"\n    loss = - 1 / 2 * torch.sum(1 + logsigma - mu ** 2 - torch.exp(logsigma))\n    return loss\n\ndef log_likelihood(x, reconstruction):\n    \"\"\"\n    часть функции потерь, которая отвечает за качество реконструкции (как mse в обычном autoencoder)\n    \"\"\"\n#     loss = nn.CrossEntropyLoss()\n#     return loss(x.flatten(1,3), reconstruction.flatten(1,3))\n    loss = nn.MSELoss(size_average=False)\n    return loss(x.flatten(1,3), reconstruction.flatten(1,3))\n\ndef loss_vae(x, mu, logsigma, reconstruction, beta):\n    return log_likelihood(x, reconstruction) + beta * KL_divergence(mu, logsigma)","metadata":{"id":"ac5ey7uIYY77","execution":{"iopub.status.busy":"2023-11-27T15:20:54.080009Z","iopub.execute_input":"2023-11-27T15:20:54.080426Z","iopub.status.idle":"2023-11-27T15:20:54.094392Z","shell.execute_reply.started":"2023-11-27T15:20:54.080397Z","shell.execute_reply":"2023-11-27T15:20:54.093555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"И обучим модель:","metadata":{"id":"ZPJQu70eYY79"}},{"cell_type":"code","source":"criterion = loss_vae\nvaencoder = VAE().to(device)\noptimizer = torch.optim.Adam(vaencoder.parameters())\n\n\nepochs = 10\nim_to_show = 5\n\ntt = []\nvv = []\n\n\nfor epoch in tqdm(range(epochs)):\n\n    show_indexes = torch.randint(10000, size=(im_to_show, ))\n    show_elems = test_dataset.data[show_indexes].to(device).float().view(im_to_show, 1, 28, 28)\n\n    t_l = []\n    v_l = []\n    \n    for k, loader in enumerate([train_loader, test_loader]):\n        print('---', epoch, k)\n        for batch, label in loader:\n            batch = batch.to(device)\n            if k == 0:\n                vaencoder.train()\n                optimizer.zero_grad()\n                mu, logsigma, reconstruction = vaencoder(batch)\n                loss = criterion(batch, mu, logsigma, reconstruction, 1)\n                t_l.append(loss.item())\n                loss.backward()\n                optimizer.step()\n\n            if k == 1:\n                with torch.no_grad():\n                    vaencoder.eval()\n                    mu, logsigma, reconstruction = vaencoder(batch)\n                    loss = criterion(batch, mu, logsigma, reconstruction, 1)\n                    v_l.append(loss.item())\n\n    tt.append(np.mean(t_l))\n    vv.append(np.mean(v_l))\n\n    with torch.no_grad():\n        autoencoder.eval()\n\n        _, _, rec = vaencoder(show_elems)\n\n        plt.figure(figsize=(5,2))\n        for i in range(im_to_show):\n            plt.subplot(2, im_to_show, i + 1)\n            plt.imshow(np.array(show_elems[i][0].cpu()))\n            plt.subplot(2, im_to_show, i + im_to_show + 1)\n            plt.imshow(np.array(rec[i][0].cpu()))\n\n        display.clear_output(wait=True)\n        display.display(plt.gcf())","metadata":{"scrolled":true,"id":"rY1khca6YY7_","outputId":"dd8f7a22-c4bd-4eec-f4e9-5cfa80cc885f","execution":{"iopub.status.busy":"2023-11-27T17:12:24.041311Z","iopub.execute_input":"2023-11-27T17:12:24.042209Z","iopub.status.idle":"2023-11-27T17:16:01.197401Z","shell.execute_reply.started":"2023-11-27T17:12:24.042176Z","shell.execute_reply":"2023-11-27T17:16:01.196404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:","metadata":{"id":"SkxW_8fkYY8B"}},{"cell_type":"markdown","source":"Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:","metadata":{"id":"PQXYIXjoYY8F"}},{"cell_type":"code","source":"# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\nz = torch.normal(mean=0, std=1, size=(10, 64, )).to(device)\nvaencoder.eval()\npred = vaencoder.decode(z)\n\n\nfor i in range(10):\n    plt.subplot(1, 10, i + 1)\n    plt.imshow(pred[i][0].cpu().detach().numpy())","metadata":{"id":"bOhhH-osYY8G","execution":{"iopub.status.busy":"2023-11-27T17:16:22.836114Z","iopub.execute_input":"2023-11-27T17:16:22.837165Z","iopub.status.idle":"2023-11-27T17:16:24.064061Z","shell.execute_reply.started":"2023-11-27T17:16:22.837131Z","shell.execute_reply":"2023-11-27T17:16:24.062989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. Latent Representation (2 балла)","metadata":{"id":"Nzt-ENxCr6ul"}},{"cell_type":"markdown","source":"Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\nВаша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве.\n\nЭто позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве.\n\nПлюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n\nПодсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n\n\nИтак, план:\n1. Получить латентные представления картинок тестового датасета\n2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр.","metadata":{"id":"uIWy670xr-Uv"}},{"cell_type":"code","source":"from sklearn.manifold import TSNE","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:21:21.062086Z","iopub.status.idle":"2023-11-27T15:21:21.062548Z","shell.execute_reply.started":"2023-11-27T15:21:21.062319Z","shell.execute_reply":"2023-11-27T15:21:21.062341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teda = test_dataset.data\ntela = test_dataset.targets\nsize = len(tela)\nindexes = [\n    (tela == i).nonzero().view(-1) for i in range(10)\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:21:21.063905Z","iopub.status.idle":"2023-11-27T15:21:21.064264Z","shell.execute_reply.started":"2023-11-27T15:21:21.064093Z","shell.execute_reply":"2023-11-27T15:21:21.064112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    autoencoder.eval()\n    \n    for n, ind in enumerate(indexes):\n        values = teda[ind][:, None, :, :].to(device)\n        sample = autoencoder.gaussian_sampler(*autoencoder.encode(values.float())).cpu().detach().numpy()\n        sample_embadded = TSNE(2).fit_transform(sample)\n        plt.figure(figsize=(5, 5))\n        plt.scatter(sample_embadded[:, 0], sample_embadded[:, 1], alpha=0.2)\n        plt.show()\n        ","metadata":{"id":"Bk94C6mCsx9c","execution":{"iopub.status.busy":"2023-11-27T15:21:21.065550Z","iopub.status.idle":"2023-11-27T15:21:21.065866Z","shell.execute_reply.started":"2023-11-27T15:21:21.065713Z","shell.execute_reply":"2023-11-27T15:21:21.065727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Что вы думаете о виде латентного представления?","metadata":{"id":"ifxhsvPss5h_"}},{"cell_type":"markdown","source":"__Congrats v2.0!__","metadata":{"id":"ESPBHrL3YY8H"}},{"cell_type":"markdown","source":"## 2.3. Conditional VAE (6 баллов)\n","metadata":{"id":"yIYuKFwijN2U"}},{"cell_type":"markdown","source":"Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер.\nДавайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица).\nИ вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n\nХотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n\nИ в этой части задания мы научимся такие обучать.","metadata":{"id":"c5l8Bu1RPjUx"}},{"cell_type":"markdown","source":"### Архитектура\n\nНа картинке ниже представлена архитектура простого Conditional VAE.\n\nПо сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки.\n\nТо есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе.","metadata":{"id":"0j8zNIwKPY-6"}},{"cell_type":"markdown","source":"\n![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n\n![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n\n","metadata":{"id":"Y6YloFEAPeM4"}},{"cell_type":"markdown","source":"На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma","metadata":{"id":"hxg2tDSfRbLF"}},{"cell_type":"markdown","source":"Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки.","metadata":{"id":"GpFbSXLaPrm1"}},{"cell_type":"markdown","source":"P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе.","metadata":{"id":"cX0zxklMPwI2"}},{"cell_type":"code","source":"class_dim = 10\nin_feat = 28 * 28\nlatent_dim = 64","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:21:21.066835Z","iopub.status.idle":"2023-11-27T15:21:21.067182Z","shell.execute_reply.started":"2023-11-27T15:21:21.066996Z","shell.execute_reply":"2023-11-27T15:21:21.067011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CVAE(nn.Module):\n    def __init__(self):\n        # <определите архитектуры encoder и decoder\n        # помните, у encoder должны быть два \"хвоста\",\n        # т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n        super().__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(in_feat + class_dim, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Linear(256, latent_dim * 2)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim + class_dim, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Linear(256, in_feat)\n        )\n        \n    def get_class_vector(self, num, class_num):\n        clazz = torch.zeros(num, class_dim)\n        for i in range(num):\n            clazz[i, class_num[i]] = 1\n        return clazz.float().to(device)\n\n    def encode(self, x, class_num):\n        x = torch.cat([\n            x.flatten(1),\n            self.get_class_vector(x.shape[0], class_num)\n        ], dim=1)\n        \n        x = self.encoder(x).view(-1, 2, latent_dim)\n\n        mu = x[:, 0, :]\n        logsigma = x[:, 1, :]\n\n        return mu, logsigma\n\n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            std = torch.exp(0.5 * logsigma)\n            eps = torch.randn_like(std)\n            return eps * std + mu\n        else:\n            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu.\n            # на инференсе выход автоэнкодера должен быть детерминирован.\n            return mu\n\n    def decode(self, z, class_num):\n        z = torch.cat([\n            z,\n            self.get_class_vector(z.shape[0], class_num)\n        ], dim=1)\n        \n        reconstruction = self.decoder(z).view(-1, 1, 28, 28)\n\n        return reconstruction\n\n    def forward(self, x, class_num):\n        mu, logsigma = self.encode(x, class_num)\n        sample = self.gaussian_sampler(mu, logsigma)\n        reconstruction = self.decode(sample, class_num)\n        return mu, logsigma, reconstruction","metadata":{"id":"ar701cHOkDKS","execution":{"iopub.status.busy":"2023-11-27T15:21:21.069571Z","iopub.status.idle":"2023-11-27T15:21:21.069909Z","shell.execute_reply.started":"2023-11-27T15:21:21.069744Z","shell.execute_reply":"2023-11-27T15:21:21.069760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = loss_vae\nautoencoder = CVAE().to(device)\noptimizer = torch.optim.Adam(autoencoder.parameters())\n\n\nepochs = 5\nim_to_show = 5\n\ntt = []\nvv = []\n\n\nfor epoch in tqdm(range(epochs)):\n\n    show_indexes = torch.randint(10000, size=(im_to_show, ))\n    show_elems = test_dataset.data[show_indexes].to(device).float().view(im_to_show, 1, 28, 28)\n    show_ind = test_dataset.targets[show_indexes].to(device)\n\n    t_l = []\n    v_l = []\n    \n    for k, loader in enumerate([train_loader, test_loader]):\n        print('---', epoch, k)\n        for batch, label in loader:\n            batch = batch.to(device)\n            if k == 0:\n                autoencoder.train()\n                optimizer.zero_grad()\n                mu, logsigma, reconstruction = autoencoder(batch, label)\n                loss = criterion(batch, mu, logsigma, reconstruction, 1)\n                t_l.append(loss.item())\n                loss.backward()\n                optimizer.step()\n\n            if k == 1:\n                with torch.no_grad():\n                    autoencoder.eval()\n                    mu, logsigma, reconstruction = autoencoder(batch, label)\n                    loss = criterion(batch, mu, logsigma, reconstruction, 1)\n                    v_l.append(loss.item())\n\n    tt.append(np.mean(t_l))\n    vv.append(np.mean(v_l))\n\n    with torch.no_grad():\n        autoencoder.eval()\nf\n        _, _, rec = autoencoder(show_elems, show_ind)\n\n        plt.figure(figsize=(5,2))\n        for i in range(im_to_show):\n            plt.subplot(2, im_to_show, i + 1)\n            plt.imshow(np.array(show_elems[i][0].cpu()))\n            plt.subplot(2, im_to_show, i + im_to_show + 1)\n            plt.imshow(np.array(rec[i][0].cpu()))\n\n        display.clear_output(wait=True)\n        display.display(plt.gcf())","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:21:21.070857Z","iopub.status.idle":"2023-11-27T15:21:21.071212Z","shell.execute_reply.started":"2023-11-27T15:21:21.071017Z","shell.execute_reply":"2023-11-27T15:21:21.071031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sampling\n","metadata":{"id":"VoMw-IFyP5A2"}},{"cell_type":"markdown","source":"Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\nДля MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7.","metadata":{"id":"qe1zWyZHkLV2"}},{"cell_type":"code","source":"with torch.no_grad():\n    autoencoder.eval()\n    pred = autoencoder.decode(\n        torch.normal(mean=0, std=1, size=(10, latent_dim)).to(device), \n        torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n#         torch.ones(10).int()\n    )\n\n\n    for i in range(10):\n        plt.subplot(1, 10, i + 1)\n        plt.imshow(pred[i][0].cpu().detach().numpy())","metadata":{"id":"A0SQIhvNP9Dr","execution":{"iopub.status.busy":"2023-11-27T15:21:21.072462Z","iopub.status.idle":"2023-11-27T15:21:21.072831Z","shell.execute_reply.started":"2023-11-27T15:21:21.072635Z","shell.execute_reply":"2023-11-27T15:21:21.072650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splendid! Вы великолепны!\n","metadata":{"id":"nAWBu8rzQBgQ"}},{"cell_type":"markdown","source":"# BONUS 1: Denoising\n\n## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя.","metadata":{"id":"KN3D_k5W_WZz"}},{"cell_type":"markdown","source":"У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них эта бонусная часть задания.","metadata":{"id":"12a1jkpkCsIU"}},{"cell_type":"markdown","source":"Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума.\nТо есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка.","metadata":{"id":"v8EN-8jlCtmd"}},{"cell_type":"markdown","source":"<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>","metadata":{"id":"j1OJg6jhlaZl"}},{"cell_type":"markdown","source":"Для этого нужно взять ваш любимый датасет (датасет лиц из первой части этого задания или любой другой) и сделать копию этого датасета с шумом.\n\nВ питоне шум можно добавить так:","metadata":{"id":"ysI0BCuRDbvm"}},{"cell_type":"code","source":"np.random.shuffle(backup)\n\ndata = torch.Tensor(backup).permute(0, 3, 1, 2).float()\n\ndata_size = data.shape[0]\ntrain_size = data_size // 10 * 7\n\ntrain_data = torch.FloatTensor(data[0:train_size]).to(device)\nval_data = torch.FloatTensor(data[train_size:data_size]).to(device)\n\nnoise_factor = 0.5\n\ntrain_noisy = train_data + noise_factor * torch.normal(mean=0, std=1, size=train_data.shape).to(device)\nval_noisy = val_data + noise_factor * torch.normal(mean=0, std=1, size=val_data.shape).to(device)\n\ntn_loader = data_utils.DataLoader(\n    torch.cat([\n        train_noisy[:, None, :, :, :],\n        train_data[:, None, :, :, :]\n    ], dim=1),\n    batch_size=64)\n\nvn_loader = data_utils.DataLoader(\n    torch.cat([\n        val_noisy[:, None, :, :, :],\n        val_data[:, None, :, :, :]\n    ], dim=1),\n    batch_size=64)","metadata":{"id":"X5e746iVDgSm","execution":{"iopub.status.busy":"2023-11-27T16:24:13.476565Z","iopub.execute_input":"2023-11-27T16:24:13.476929Z","iopub.status.idle":"2023-11-27T16:24:15.456493Z","shell.execute_reply.started":"2023-11-27T16:24:13.476900Z","shell.execute_reply":"2023-11-27T16:24:15.455406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:24:15.458160Z","iopub.execute_input":"2023-11-27T16:24:15.458436Z","iopub.status.idle":"2023-11-27T16:24:15.465777Z","shell.execute_reply.started":"2023-11-27T16:24:15.458412Z","shell.execute_reply":"2023-11-27T16:24:15.464855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\n\nclass Denoiser(nn.Module):\n    def __init__(self):\n\n        super().__init__()\n\n        self.encoder = nn.Sequential(\n            # 64\n            nn.Conv2d(3, 16, kernel_size=3), # 62\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d(2), # 31\n            nn.Conv2d(16, 32, kernel_size=4), # 28\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2), # 14\n            nn.Conv2d(32, 64, kernel_size=3), # 12\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2), # 6\n            nn.Conv2d(64, 64, kernel_size=3), # 4\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2), # 2\n        )\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1), # 4\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=1), # 7\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), # 14\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.ConvTranspose2d(16, 8, kernel_size=5, stride=2), # 31\n            nn.ReLU(),\n            nn.BatchNorm2d(8),\n            nn.ConvTranspose2d(8, 3, kernel_size=4, stride=2), # 64\n        )\n\n    def forward(self, x):\n\n        x = self.encoder(x)\n        latent_code = x\n\n        x = self.decoder(x)\n        return x, latent_code","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:24:44.315907Z","iopub.execute_input":"2023-11-27T16:24:44.316656Z","iopub.status.idle":"2023-11-27T16:24:44.328177Z","shell.execute_reply.started":"2023-11-27T16:24:44.316623Z","shell.execute_reply":"2023-11-27T16:24:44.327082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:20:29.259739Z","iopub.execute_input":"2023-11-27T16:20:29.260159Z","iopub.status.idle":"2023-11-27T16:20:29.267022Z","shell.execute_reply.started":"2023-11-27T16:20:29.260119Z","shell.execute_reply":"2023-11-27T16:20:29.266058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:14:00.909633Z","iopub.execute_input":"2023-11-27T16:14:00.910318Z","iopub.status.idle":"2023-11-27T16:14:13.762324Z","shell.execute_reply.started":"2023-11-27T16:14:00.910284Z","shell.execute_reply":"2023-11-27T16:14:13.761269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:15:31.296585Z","iopub.execute_input":"2023-11-27T16:15:31.297514Z","iopub.status.idle":"2023-11-27T16:15:31.307275Z","shell.execute_reply.started":"2023-11-27T16:15:31.297477Z","shell.execute_reply":"2023-11-27T16:15:31.306280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()\nautoencoder = Denoiser().to(device)\noptimizer = optim.Adam(autoencoder.parameters())\n\n\nsummary(autoencoder, (3, 64, 64))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:25:40.214424Z","iopub.execute_input":"2023-11-27T16:25:40.214798Z","iopub.status.idle":"2023-11-27T16:25:40.235607Z","shell.execute_reply.started":"2023-11-27T16:25:40.214768Z","shell.execute_reply":"2023-11-27T16:25:40.234686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 15\nim_to_show = 5\n\nt_losses = []\nv_losses = []\n\nshow_indexes = torch.randint(3945, size=(im_to_show, ))\nshow_sample = val_data[show_indexes]\nshow_noisy = val_noisy[show_indexes]\n\nfor epoch in tqdm(range(epochs)):\n    t_l = []\n    v_l = []\n    for k, loader in enumerate([tn_loader, vn_loader]):\n        for element in loader:\n            element = element.to(device)\n            \n            real = element[:, 1].view(-1, 3, 64, 64)\n            noisy = element[:, 0].view(-1, 3, 64, 64)\n            \n            \n            if k == 0:\n                autoencoder.train()\n                optimizer.zero_grad()\n                pred, _ = autoencoder(noisy)\n                loss = criterion(pred.flatten(1), real.flatten(1))\n                # print(loss.item())\n                t_l.append(loss.item())\n                loss.backward()\n                optimizer.step()\n\n            if k == 1:\n                with torch.no_grad():\n                    autoencoder.eval()\n                    pred, _ = autoencoder(noisy)\n                    loss = criterion(pred.flatten(1), real.flatten(1))\n                    v_l.append(loss.item())\n\n\n    t_losses.append(np.mean(t_l))\n    v_losses.append(np.mean(v_l))\n    \n    if epoch % 5 == 0:\n\n        with torch.no_grad():\n            autoencoder.eval()\n\n            pred, _ = autoencoder(show_noisy.to(device))\n\n            plt.figure(figsize=(10,3))\n\n            for i in range(im_to_show):\n                plt.subplot(3, im_to_show, i + 1)\n                plt.imshow(show_sample[i].permute(1, 2, 0).cpu().detach().numpy())\n\n                plt.subplot(3, im_to_show, i + im_to_show + 1)\n                plt.imshow(show_noisy[i].permute(1, 2, 0).cpu().detach().numpy())\n\n                plt.subplot(3, im_to_show, i + 2 * im_to_show + 1)\n                plt.imshow(pred[i].permute(1, 2, 0).cpu().detach().numpy())\n\n            display.clear_output(wait=True)\n            display.display(plt.gcf())\n\n# plt.figure(figsize=(12,6))\n# plt.plot(range(epochs), t_losses, label='train')\n# plt.plot(range(epochs), v_losses, label='val')\n# plt.legend()","metadata":{"id":"9fSPkXMtDpd5","execution":{"iopub.status.busy":"2023-11-27T16:27:30.855876Z","iopub.execute_input":"2023-11-27T16:27:30.856320Z","iopub.status.idle":"2023-11-27T16:27:57.048960Z","shell.execute_reply.started":"2023-11-27T16:27:30.856272Z","shell.execute_reply":"2023-11-27T16:27:57.047952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    autoencoder.eval()\n    a, _ = autoencoder(val_noisy[6:11])\n    for i in range(5):\n        plt.figure(figsize=(2,2))\n        plt.imshow(a[i].permute(1, 2, 0).cpu().detach().numpy())\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T17:28:37.249536Z","iopub.execute_input":"2023-11-27T17:28:37.250001Z","iopub.status.idle":"2023-11-27T17:28:39.049357Z","shell.execute_reply.started":"2023-11-27T17:28:37.249971Z","shell.execute_reply":"2023-11-27T17:28:39.048147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"B03NQ_sKDvg2","execution":{"iopub.status.busy":"2023-11-27T15:21:21.079220Z","iopub.status.idle":"2023-11-27T15:21:21.079579Z","shell.execute_reply.started":"2023-11-27T15:21:21.079402Z","shell.execute_reply":"2023-11-27T15:21:21.079419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BONUS 2: Image Retrieval\n\n## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя.","metadata":{"id":"-NDiCPYLm2bY"}},{"cell_type":"markdown","source":"Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!","metadata":{"id":"xao_27WMm7AL"}},{"cell_type":"markdown","source":"План:\n\n1. Получаем латентные представления всех лиц тренировочного датасета\n2. Обучаем на них LSHForest `(sklearn.neighbors.LSHForest)`, например, с `n_estimators=50`\n3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!","metadata":{"id":"Y__bdS23ndeY"}},{"cell_type":"markdown","source":"Немного кода вам в помощь: (feel free to delete everything and write your own)","metadata":{"id":"IksC2ucIoND-"}},{"cell_type":"code","source":"codes = autoencoder.encoder(train_data)","metadata":{"id":"hK0YpLMRoEa0","execution":{"iopub.status.busy":"2023-11-27T17:22:21.106693Z","iopub.execute_input":"2023-11-27T17:22:21.107479Z","iopub.status.idle":"2023-11-27T17:22:21.306787Z","shell.execute_reply.started":"2023-11-27T17:22:21.107442Z","shell.execute_reply":"2023-11-27T17:22:21.305908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обучаем LSHForest\nfrom sklearn.neighbors import LSHForest\nlshf = LSHForest(n_estimators=50).fit(codes)","metadata":{"id":"KisDrgZdoWdt","execution":{"iopub.status.busy":"2023-11-27T17:22:28.285224Z","iopub.execute_input":"2023-11-27T17:22:28.285988Z","iopub.status.idle":"2023-11-27T17:22:28.632509Z","shell.execute_reply.started":"2023-11-27T17:22:28.285954Z","shell.execute_reply":"2023-11-27T17:22:28.631151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_similar(image, n_neighbors=5):\n  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n  # прогоняет векторы через декодер и получает картинки ближайших людей\n\n  code = <получение латентного представления image>\n\n  (distances,),(idx,) = lshf.kneighbors(code, n_neighbors=n_neighbors)\n\n  return distances, X_train[idx]","metadata":{"id":"Y_S5zPb5obam","execution":{"iopub.status.busy":"2023-11-27T15:21:21.083993Z","iopub.status.idle":"2023-11-27T15:21:21.084332Z","shell.execute_reply.started":"2023-11-27T15:21:21.084173Z","shell.execute_reply":"2023-11-27T15:21:21.084188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_similar(image):\n\n  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n\n    distances,neighbors = get_similar(image,n_neighbors=11)\n\n    plt.figure(figsize=[8,6])\n    plt.subplot(3,4,1)\n    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n    plt.title(\"Original image\")\n\n    for i in range(11):\n        plt.subplot(3,4,i+2)\n        plt.imshow(neighbors[i].cpu().numpy().transpose([1,2,0]))\n        plt.title(\"Dist=%.3f\"%distances[i])\n    plt.show()","metadata":{"id":"t2kjV5wupLP_","execution":{"iopub.status.busy":"2023-11-27T15:21:21.085350Z","iopub.status.idle":"2023-11-27T15:21:21.085797Z","shell.execute_reply.started":"2023-11-27T15:21:21.085564Z","shell.execute_reply":"2023-11-27T15:21:21.085586Z"},"trusted":true},"execution_count":null,"outputs":[]}]}